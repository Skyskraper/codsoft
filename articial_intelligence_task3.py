# -*- coding: utf-8 -*-
"""articial_intelligence_task3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1viss14vwqOguOjpvsWt9-ewY8-XVkkS8
"""

from typing import List, Tuple, Optional
import math
import os
import random


import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import models, transforms
from PIL import Image

def seed_everything(seed: int = 42):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


seed_everything()

class SimpleImageCaptionDataset(Dataset):
    """A tiny example dataset: list of (image_path, caption_tokens).


    caption_tokens should be already tokenized integer token ids.
    This keeps dataset code separate from tokenizer choices.
    """
    def __init__(self, rows: List[Tuple[str, List[int]]], transform=None):
        self.rows = rows
        self.transform = transform


    def __len__(self):
        return len(self.rows)


    def __getitem__(self, idx):
        img_path, token_ids = self.rows[idx]
        img = Image.open(img_path).convert('RGB')
        if self.transform:
            img = self.transform(img)
        tokens = torch.tensor(token_ids, dtype=torch.long)
        return img, tokens

class ResNetEncoder(nn.Module):
    """Pretrained ResNet backbone that outputs spatial features.


    It returns a tensor of shape (batch, num_patches, enc_dim)
    where num_patches = H'*W' (e.g., 7x7 = 49) and enc_dim = 2048
    for ResNet-50.
    """
    def __init__(self, model_name: str = 'resnet50', pretrained: bool = True):
        super().__init__()
        if model_name != 'resnet50':
            raise NotImplementedError('Only resnet50 implemented in this starter')
        backbone = models.resnet50(pretrained=pretrained)
        # Remove avgpool and fc layers
        modules = list(backbone.children())[:-2]
        self.body = nn.Sequential(*modules)
        self.enc_dim = 2048


    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (B, 3, H, W)
        feat = self.body(x)
        # feat: (B, C=2048, H', W')
        B, C, H, W = feat.shape
        feat = feat.view(B, C, H * W).permute(0, 2, 1) # (B, N, C)
        return feat

class TransformerCaptionDecoder(nn.Module):
    """A Transformer-based decoder that attends to encoder features.


    This is a simplified decoder using PyTorch's nn.Transformer.
    """
    def __init__(self,
                 vocab_size: int,
                 d_model: int = 512,
                 nhead: int = 8,
                 num_layers: int = 3,
                 dim_feedforward: int = 2048,
                 dropout: float = 0.1,
                 enc_dim: int = 2048,
                 max_len: int = 50):
        super().__init__()
        self.d_model = d_model
        self.token_embed = nn.Embedding(vocab_size, d_model)
        self.pos_embed = nn.Embedding(max_len, d_model)
        # Project encoder features to d_model
        self.enc_proj = nn.Linear(enc_dim, d_model)


        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model,
                                                   nhead=nhead,
                                                   dim_feedforward=dim_feedforward,
                                                   dropout=dropout)
        self.transformer_decoder = nn.TransformerDecoder(decoder_layer,
                                                         num_layers=num_layers)
        self.output_fc = nn.Linear(d_model, vocab_size)


    def forward(self,
                tgt_tokens: torch.Tensor,
                memory: torch.Tensor,
                tgt_mask: Optional[torch.Tensor] = None,
                tgt_key_padding_mask: Optional[torch.Tensor] = None):
        # tgt_tokens: (T, B) or (B, T) - we'll use (T, B) for transformer
        if tgt_tokens.dim() == 2:
            # expecting (T, B)
            tgt = self.token_embed(tgt_tokens) * math.sqrt(self.d_model) # (T, B, d)
        else:
            raise ValueError('tgt_tokens must be (T, B)')
        T, B, _ = tgt.shape
        positions = torch.arange(0, T, device=tgt_tokens.device).unsqueeze(1)
        pos_emb = self.pos_embed(positions) # (T, 1, d)
        tgt = tgt + pos_emb


        # memory: (B, N, enc_dim) -> (N, B, d_model)
        mem = self.enc_proj(memory) # (B, N, d_model)
        mem = mem.permute(1, 0, 2)


        # run Transformer decoder
        out = self.transformer_decoder(tgt, mem,
                                       tgt_mask=tgt_mask,
                                       tgt_key_padding_mask=tgt_key_padding_mask)
        logits = self.output_fc(out) # (T, B, vocab_size)
        return logits

class ImageCaptioningModel(nn.Module):
    def __init__(self, vocab_size: int, **decoder_kwargs):
        super().__init__()
        self.encoder = ResNetEncoder()
        self.decoder = TransformerCaptionDecoder(vocab_size=vocab_size,
                                                 enc_dim=self.encoder.enc_dim,
                                                 **decoder_kwargs)


    def forward(self, images: torch.Tensor, tgt_tokens: torch.Tensor,
                tgt_mask: Optional[torch.Tensor] = None,
                tgt_key_padding_mask: Optional[torch.Tensor] = None):
        # images: (B,3,H,W)
        enc = self.encoder(images) # (B, N, C)
        # rearrange tgt for transformer: (T, B)
        tgt = tgt_tokens.permute(1, 0)
        logits = self.decoder(tgt, enc, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask)
        return logits

def subsequent_mask(size: int) -> torch.Tensor:
    """Create square causal mask for target sequences (T, T)"""
    attn_shape = (size, size)
    subsequent = torch.triu(torch.ones(attn_shape), diagonal=1).bool()
    mask = subsequent
    return mask

def train_one_epoch(model: nn.Module, dataloader: DataLoader, optimizer, device, pad_token: int):
    model.train()
    total_loss = 0.0
    for images, tokens in dataloader:
        images = images.to(device)
        tokens = tokens.to(device) # (B, T)
        B, T = tokens.size()
        # input to decoder: all tokens except last
        inp = tokens[:, :-1]
        tgt = tokens[:, 1:]
        inp_t = inp.permute(1, 0) # (T-1, B)
        tgt_t = tgt.permute(1, 0)


        tgt_mask = subsequent_mask(inp_t.size(0)).to(device)
        # create padding mask (B, T-1) if pad token present
        pad_mask = (inp == pad_token)


        logits = model(images, inp_t, tgt_mask=tgt_mask, tgt_key_padding_mask=pad_mask)
        # logits: (T-1, B, V)
        logits_flat = logits.view(-1, logits.size(-1))
        tgt_flat = tgt_t.reshape(-1)


        loss = F.cross_entropy(logits_flat, tgt_flat, ignore_index=pad_token)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * B
    return total_loss / len(dataloader.dataset)

def generate_caption(model: nn.Module, image: torch.Tensor, tokenizer, device,
                     max_len: int = 30, sos_token: int = 1, eos_token: int = 2):
    model.eval()
    with torch.no_grad():
        image = image.to(device).unsqueeze(0) # (1,3,H,W)
        enc = model.encoder(image)
        enc = enc.to(device)


        generated = [sos_token]
        for i in range(max_len):
            inp = torch.tensor(generated, dtype=torch.long, device=device).unsqueeze(1) # (T,1)
            tgt_mask = subsequent_mask(inp.size(0)).to(device)
            logits = model.decoder(inp, enc, tgt_mask=tgt_mask)
            # take last timestep
            next_logits = logits[-1, 0, :]
            next_token = torch.argmax(next_logits).item()
            generated.append(next_token)
            if next_token == eos_token:
                break
        return tokenizer.decode(generated)

if __name__ == '__main__':
    print('This file is a starter template for an image captioning project.')
    print('Edit the dataset-loading section to connect a tokenizer and COCO or a small dataset.')
    print('\nRecommended next steps:')
    print('1) Choose a tokenizer: simple whitespace + special tokens OR use "transformers" Tokenizer.')
    print('2) Prepare COCO 2017 captions or a small dataset and implement collate_fn to pad sequences.')
    print('3) Train for multiple epochs and evaluate with BLEU/CIDEr metrics.')
    print('4) Add learning rate scheduler and gradient clipping for stability.')


    # quick smoke test (requires at least one image "sample.jpg" and a tokenizer stub)
    sample_image_path = 'sample.jpg'
    if os.path.exists(sample_image_path):
        print('\nFound sample.jpg — running a quick forward pass...')
        # minimal tokenizer stub
        class StubTok:
            def decode(self, ids):
                return ' '.join(str(i) for i in ids)
        tok = StubTok()


        transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        img = Image.open(sample_image_path).convert('RGB')
        timg = transform(img)


        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model = ImageCaptioningModel(vocab_size=1000).to(device)
        out = model(timg.unsqueeze(0).to(device), torch.randint(0, 1000, (1, 10)).to(device))
        print('Forward pass OK — logits shape:', out.shape)
    else:
        print('\nNo sample.jpg found — place one in the working folder to run a smoke test.')